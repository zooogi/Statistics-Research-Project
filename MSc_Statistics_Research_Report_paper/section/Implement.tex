\begin{figure}[H]
\centering
\resizebox{0.85\linewidth}{!}{
\begin{tikzpicture}[
  node distance=12mm,
  box/.style      ={rectangle, draw, rounded corners, align=center,
                    minimum width=46mm, minimum height=10mm},
  decision/.style ={diamond, aspect=2.2, draw, align=center, inner sep=1.5pt},
  ->, >=Latex
]

%--- 主流程节点 ------------------------------------------------------
\node[box]      (model)   {Define a model};
\node[box]      (checking1)  [below=of model] {model checking \\(On simulated data)};
\node[decision] (pass1)   [below=10mm of checking1] {Fits?};
\node[box]      (fit)     [below=of pass1]  {Fit to real data\\$\Rightarrow$ posterior};
\node[box]      (gen)     [below=of fit]    {Generate fake data\\via posterior};
\node[box]      (compare) [right=of gen]    {Compare fake vs real\\(posterior predictive model checking)};
%\node[decision] (pass2)   [below=of compare] {Adequate?};
%\node[box]      (report)  [below=20mm of pass2] {Report / interpret};

%--- 连线 ------------------------------------------------------------
\draw (model)   -- (checking1)
      (checking1)  -- (pass1)
      (pass1)   -- node[right]{Yes}(fit)
      (fit)     -- (gen)
      (gen)     -- (compare);
      %(compare) -- (pass2)
      %(pass2)   -- node[right]{Yes}(report);

%--- 回环：不合格则返回模型阶段 ------------------------------------
\draw[->] (pass1.west) -- ++(-30mm,0) |- (model.west) node[pos=0.33, left]{No};
%\draw[->] (pass1.west) to[out=180,in=180,looseness=1.3] node[left]{No} (model.west);
%\draw[->] (pass2.east) to[out=0,in=0,looseness=1.3]   node[right]{No} (model.east);
\end{tikzpicture}}
\caption{General Bayesian workflow}
\end{figure}

\begin{tcolorbox}[
  title   = \textbf{Algorithm --Simulating a Fake Survival Dataset (e.g. Posterior predictive model checking)},
   fonttitle  = \small, 
  colback = white,
  colframe= black,
  breakable ]
\textbf{Input}\\
\quad$\bullet$ posterior samples $\{\lambda^{(s)}\}$ \hfill (obtained by fitting real data)\\
\quad$\bullet$ sample size $n$ \hfill (same size as the real data set)\\
\quad$\bullet$ start‑time range $[a,0]$ with $a<0$ \hfill (e.g.\ $a=-150$)\\[6pt]

\textbf{Algorithm}\par
\begin{enumerate}
  \item Choose a single $\lambda^\ast$ from posterior samples 
  \item For $i = 1,\dots,n$
        \begin{enumerate}
          \item[] \hspace*{-10pt}%
          \begin{minipage}[t]{\linewidth}
          \begin{enumerate}
            \item Draw latent duration: $y_i \sim \operatorname{Exp}(\lambda^\ast)$
            \item Draw start time: $T_i \sim \operatorname{Uniform}(a,0)$
            \item Compute leaving time: $t_i = T_i + y_i$
            \item Observed pair $(\text{time}_i,\text{event}_i)$
                  \begin{lstlisting}[numbers=none,frame=none]
if t_i < 0:         # event occurred before now
    event_i = 1     # uncensored
    time_i  = y_i
else:               # event in the future
    event_i = 0     # right censored
    time_i  = -T_i  # time already spent
\end{lstlisting}
          \end{enumerate}
          \end{minipage}
        \end{enumerate}
  \item Combine $(\text{time}_i,\text{event}_i)$ into a fake data set of size $n$.
\end{enumerate}
\end{tcolorbox}


To evaluate how well the model captures the distribution of event times, a common approach is to compare the posterior predictive density with the histogram of observed events. While intuitive, histogram-based comparisons suffer from several drawbacks: they depend on arbitrary binning, are sensitive to sample size and distribution, and reflect local density rather than cumulative structure—potentially leading to misleading conclusions.

To overcome these limitations, we use the empirical cumulative distribution function (ECDF) of observed events, which avoids binning, retains full order information, and remains stable under small sample sizes. We then compare this ECDF to the model’s posterior predictive cumulative distribution function (CDF) to assess overall fit.

The posterior predictive density is defined as
\begin{equation}
    p(\tilde{y} \mid D) = \int p(\tilde{y} \mid \theta) \, p(\theta \mid D) \, d\theta,
\end{equation}
When defining the posterior predictive cumulative distribution function (CDF), we start from the definition
\begin{equation}
F_{\text{post}}(t)
= P(\tilde{y} \le t \mid D)
= \int_0^{t} p_{\text{post}}(s) ds, \quad t\ge 0
\end{equation}
where $p_{\text{post}}(s)$ is the posterior predictive probability density function (PDF)
\begin{equation}
p_{\text{post}}(s)
= \int f_T(s\mid\theta)\,p(\theta \mid D) d\theta
\end{equation}
By substituting $p_{\text{post}}(s)$ into the expression for $F_{\text{post}}(t)$ and exchanging the order of integration (by Fubini's theorem), we obtain
\begin{align}
F_{\text{post}}(t)
&= \int_0^{t} 
\left[ \int f_T(s\mid\theta)\,p(\theta \mid D) d\theta \right] ds \\[6pt]
&= \int \left[ \int_0^{t} f_T(s\mid\theta)\, ds \right] p(\theta \mid D) d\theta \\[6pt]
&= \int F_T(t\mid\theta)\, p(\theta \mid D) d\theta,
\label{eq:25}
\end{align}
where $F_T(t\mid\theta)$ denotes the theoretical CDF given parameter $\theta$.

Since the integral in Equation~\eqref{eq:25} is usually intractable in practice, it can be approximated using Monte Carlo sampling $\theta^{(1)}, \dots, \theta^{(M)}$
\begin{equation}
    F_{\text{post}}(t) \approx \frac{1}{M} \sum_{m=1}^M F(t \mid \theta^{(m)}).
\end{equation}

For the exponential model, this can be implemented as \texttt{mean(pexp(t, rate = post\_lam))}, where \texttt{post\_lam} are posterior samples. We then plot the predictive CDF against the ECDF of observed events for visual comparison.






