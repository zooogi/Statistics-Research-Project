The analysis of survival time has a long intellectual history, from the population censuses of the Western Han dynasty in China and the “Yellow Registers” of the Ming dynasty~\cite{von2012household}, to John Graunt’s mortality tables during the London plague in the 17th century~\cite{doi:10.1177/09677720221079826}. These efforts, though rudimentary, already showed how survival data could inform decisions. Modern survival analysis, however, took shape in the mid-20th century with the emergence of parametric, nonparametric, and semiparametric approaches. Parametric models such as the exponential or Weibull~\cite{ibrahim2013bayesian} were among the earliest, assuming specific hazard forms that make inference straightforward but rely on restrictive distributional assumptions. The Kaplan–Meier estimator (1958)~\cite{liu2012survival, kleinbaum1996survival} then introduced a fully nonparametric method for censored data, providing intuitive survival curves without distributional assumptions, though it cannot incorporate covariates. The Cox proportional hazards model (1972)~\cite{Efron01091977, liu2012survival} built on this by combining covariate effects with an unspecified baseline hazard, becoming the standard in biomedical research despite its reliance on the proportional hazards assumption. Together, these models established the classical toolbox that remains central to medicine, epidemiology, and reliability engineering.

Yet this toolbox has limitations. Traditional inference via maximum or partial likelihood~\cite{bartovs2022informed, kalbfleisch2002statistical} provides only point estimates, which can be highly unstable under heavy censoring or small samples, and does not naturally convey uncertainty. While large-sample theory guarantees consistency, in practice such estimates can be misleading when event counts are low. 
Bayesian approaches~\cite{gelman1995bayesian} address these issues by delivering full posterior distributions, allowing both uncertainty quantification and the incorporation of prior knowledge. Crucially, they also enable posterior predictive model checking~\cite{gelman1995bayesian, https://doi.org/10.1002/ecm.1314}, which goes beyond assessing goodness of fit to ask whether a model reproduces the structural features of the data-generating process. This perspective is central to the present study.

One structural feature that deserves special attention is administrative censoring. In most applications, the survey or observation window is treated as an external design choice, with little attention paid to its consequences~\cite{barrajón2020effectrightcensoringbias, bartovs2022informed}. Yet model checking reveals its importance. When the window is too short, many individuals are censored prematurely, and when it is excessively long, implausible long tails emerge~\cite{barrajón2020effectrightcensoringbias}. In other words, the observation window is not a neutral background assumption, but a structural parameter whose neglect can distort inference.

To illustrate these issues, we analyze a publicly available employee turnover dataset~\cite{babushkin_employee_turnover}, recording the tenure of 1,129 employees and whether they exited during the survey period. While the dataset is not the primary object of study, it provides a concrete case where censoring and observation windows crucially shape inference. It raises three motivating questions: whether the exponential survival model with its constant hazard is adequate to explain turnover durations, how the observation window influences censoring and estimation, and how Bayesian model checking can reveal structural issues and guide model extensions. Addressing these questions, we develop a Bayesian survival framework that incorporates the observation window as an estimable parameter, studies its identifiability and posterior structure, and evaluates its role in shaping inference, while briefly outlining alternative baselines suggested by the diagnostics.
