\subsubsection{Why Are Two Layers of Model Checking Necessary}
\label{subsec:wo Layers of Model Checking}
In Section~\ref{指数模型贝叶斯推断过程}, we derived the exponential survival model under a Gamma prior, demonstrated its closed-form posterior, and confirmed the agreement between analytical and MCMC-based computation. While this validates parameter estimation under the model’s assumptions,  it does not guarantee that the model itself captures the key structural features of the observed data ~\cite{62bfc978-09b1-3997-9776-380d0b45e9c2, gelman1995bayesian} — for instance, the constant-hazard assumption of the exponential distribution may be too restrictive.

In practice, the actual data-generating process may involve additional unobserved mechanisms~\cite{kalbfleisch2002statistical}. For example, there may be a fixed observation window applied to all individuals, shaping the censoring pattern in ways the current model does not explicitly represent. Ignoring such mechanisms can lead to systematic bias in parameter estimates and distort model predictions~\cite{stats5010006}.

Before drawing inferences or making interpretations, it is therefore necessary to examine both structural validity and adequacy of fit~\cite{62bfc978-09b1-3997-9776-380d0b45e9c2}. In Bayesian modelling, the primary purpose of model checking is to assess whether the proposed model is grounded in sound assumptions and whether it can reproduce the observed data~\cite{https://doi.org/10.1002/ecm.1314}. Unlike traditional approaches that rely solely on goodness-of-fit metrics (such as the likelihood value or information criteria like WAIC or LOO)~\cite{cho2025nonlinear, https://doi.org/10.1002/ecm.1314}, model checking shifts the focus away from mere “score performance” on observed data and toward verifying whether the model captures key structural features from a generative perspective. 

As shown in Figure~\ref{flowchart}, this process reflects two fundamental validation tasks in Bayesian modelling.
\begin{enumerate}
    \item Is the model structure adequate to explain the observed data?
    \item To what extent can the model reproduce reality after inference?
\end{enumerate}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%流程图workflow%%%%%%%%%%
\begin{figure}[H]
\centering
\resizebox{0.55\linewidth}{!}{
\begin{tikzpicture}[
 scale=0.53,
  every node/.style={transform shape}, 
  node distance=11mm,
  box/.style      ={rectangle, draw, rounded corners, align=center,
                    minimum width=44mm, minimum height=9mm},
  decision/.style ={diamond, aspect=2.2, draw, align=center, inner sep=1.4pt},
  ->, >=Latex
]

%--- Main process node ------------------------------------------------------
\node[box]      (model)   {\textbf{Define/Revise a model}};

\node[box]      (checking1)  [below=8mmof model] {\textbf{Simulation-based Model Checking} \\
(Simulate with $\lambda_0$; fit model; recover $\lambda_0$?)};

\node[decision] (pass1)   [below=6mm of checking1] {\textbf{Plausible?}};

\node[box]      (fit)     [below=of pass1]  {\textbf{Fit to real data}\\
(Compute posterior)};

\node[box]      (gen)     [below=of fit]    {\textbf{Posterior Predictive Model Checking}\\
(Generate fake data from Posterior)\\
(Compare fake vs real)};

\node[decision] (pass2)   [below=6mmof gen] {\textbf{Adequate?}};

\node[box]     (report)  [below=of pass2] {\textbf{Report/Interpret}};

%--- connection ------------------------------------------------------------
\draw (model)   -- (checking1)
      (checking1)  -- (pass1)
      (pass1)   -- node[right]{Yes}(fit)
      (fit)     -- (gen)
      (gen) -- (pass2)
      (pass2) -- node[right]{Yes}(report);

%--- Loopback ------------------------------------
\draw[->] (pass1.west) -- ++(-30mm,0) |- (model.west) node[pos=0.23, left]{No};
%\draw[->] (pass1.west) to[out=180,in=180,looseness=1.3] node[left]{No} (model.west);
\draw[->] (pass2.east) -- ++ (30mm,0 )|- (model.east) node[pos=0.23,right]{No};
\end{tikzpicture}}
\caption{General Bayesian workflow}
\label{flowchart}
\end{figure}

This workflow clearly distinguishes two levels of model checking.

First is the \textbf{Simulation-based Model Checking}, which evaluates structural identifiability based on known parameters~\cite{10.1093/bioinformatics/btp358}. Before fitting any real data, we can simulate pseudo-datasets using a fixed value $\lambda_0$, and then re-fit the model using the same procedure. If we successfully “recover” $\lambda_0$, this suggests that the model structure is sound; failure to do so implies structural flaws in the model, rendering downstream inferences on real data invalid~\cite{10.1093/bioinformatics/btp358, pub.1044073403}.

Second is the \textbf{Posterior Predictive Checking}, which evaluates the adequacy of the fitted model after conditioning on real data~\cite{https://doi.org/10.1002/ecm.1314}. In this approach, samples are drawn from the posterior distribution and used to generate replicated or “fake” datasets. These are then compared against the observed data to assess whether the model captures key statistical features. The underlying principle is that if data simulated from the posterior distribution differ systematically from the observed data, for example, in terms of censoring patterns or the distribution of event times, this indicates that the model fails to represent the true underlying process~\cite{62bfc978-09b1-3997-9776-380d0b45e9c2}. Bayesian methods naturally account for uncertainty by incorporating the full posterior distribution into this checking procedure~\cite{van2021bayesian}, ensuring that the evaluation reflects the range of plausible parameter values. Crucially, the discrepancies revealed by posterior predictive checks not only highlight structural inadequacies but also motivate targeted model extensions, guiding the development of more realistic survival models.


To implement this, we design a simulation procedure to generate synthetic survival datasets that preserve the censoring mechanism of the real data (see Algorithm 1)~\cite{ashhad2025generatingaccuratesyntheticsurvival}. The algorithm proceeds by drawing a posterior sample $\lambda^*$ and defining a suitable time window $[a, 0]$ to simulate individuals' entry times and latent event durations. For tractability, we assume that entry times are uniformly distributed across the observation window, corresponding to a constant entry rate during the study period. This allows us to generate a “virtual dataset” that matches the sample size and censoring structure of the original data, thereby enabling a rigorous comparison for posterior predictive model checking.
%%%%%%算法--------------
\begin{tcolorbox}[
  title  = {Algorithm 1: Simulating a Fake Survival Dataset (e.g., Posterior predictive model checking)},
  label={fake data},
   fonttitle  = \bfseries\footnotesize,
   fontupper=\footnotesize,
    %width = 0.9\textwidth,   
  box align = center, 
  colback = white,
  colframe=black,
   boxsep  = 3pt,
  left=4pt,
   right=4pt,
  top=5pt,
  bottom=4pt]
\textbf{Input}
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item posterior samples $\{\lambda^{(s)}\}$ \hfill (obtained by fitting real data)
\item sample size $n$ \hfill (same size as the real data set)
\item start‑time range $[a,0]$ with $a<0$
\end{itemize}

\textbf{Algorithm}
\begin{enumerate}[itemsep=2pt,parsep=0pt,topsep=2pt]
  \item Choose a single $\lambda^\ast$ from posterior samples \hfill (e.g.\ posterior mean)
  \item For $i = 1,\dots,n$
          %\begin{minipage}[t]{\linewidth}
          \begin{enumerate}
            \item Draw latent duration: $y_i \sim \operatorname{Exp}(\lambda^\ast)$
            \item Draw start time: $T_i \sim \operatorname{Uniform}(a,0)$
            \item Compute leaving time: $t_i = T_i + y_i$
            \item Observed pair $(\text{time}_i,\text{event}_i)$ \[\begin{aligned}
\text{if } \quad t_i &< 0: &&&\text{(event occurred before now)}\\
  &&\quad  \text{event}_i=1, &&\text{(uncensored)}\\ 
  &&\quad \text{time}_i=y_i
   &\quad \\
&\text{else}: &&&\text{(event in the future)}\\
   &&\quad \text{event}_i=0, &&\text{(right censored)}\\ 
   &&\quad \text{time}_i=-T_i &&\text{(time already spent)}
   &\quad 
\end{aligned}\]
          \end{enumerate}
  \item Combine $(\text{time}_i,\text{event}_i)$ into a fake data set of size $n$.
\end{enumerate}
\end{tcolorbox}





\subsubsection{Model Checking via ECDF under Independent Censoring}
In model checking, to compare the overall distributional shapes of the real data and the simulated data, we use the empirical cumulative distribution function (ECDF) rather than histograms. Unlike histograms, ECDFs do not depend on subjective choices of bin widths and break points, thereby avoiding visual biases~\cite{berg2008data}. Moreover, an ECDF is a monotone right-continuous step function defined on $[0,\infty)$, which stably displays differences between samples over the entire time axis~\cite{arnold2011nonparametric, berg2008data}. Importantly, ECDFs can be plugged directly into distance statistics such as the Kolmogorov–Smirnov or Cramér–von Mises metrics~\cite{arnold2011nonparametric}, facilitating quantitative assessment of model fit.

In survival data, the observed duration is determined jointly by the latent event time $T\ge 0$ and the censoring time $C\ge 0$. The observed quantity is
$$
Y=\min(T,C),\qquad \delta=\mathbf 1\{T\le C\}.
$$
That is, we observe the event time only when it is uncensored; otherwise, we observe the censoring time. Consequently, we split the data into two subsamples, an “event subsample’’ with $\delta=1$ and a “censored subsample’’ with $\delta=0$, and compute ECDFs for each subsample separately.

Under independent (non-informative) censoring, i.e., $T\perp C$, the two subsamples may be viewed as arising from two conditional distributions~\cite{fleming2013counting}
\begin{itemize}
    \item for the event subsample ($\delta=1$), the observed values $Y=T$ follow the conditional distribution $T\mid(T\le C)$;
    \item for the censored subsample ($\delta=0$), the observed values $Y=C$ follow the conditional distribution $C\mid(C<T)$.
\end{itemize}
Let the event-subsample size be $n_1=\sum_i \delta_i$ and the censored-subsample size be $n_0=\sum_i (1-\delta_i)$. The corresponding empirical distribution functions are~\cite{fleming2013counting, arnold2011nonparametric}
\begin{equation}
    \widehat H_{\text{event}}(t)
=\frac{1}{n_1}\sum_{i:\,\delta_i=1}\mathbf 1\{Y_i\le t\},\qquad
\widehat H_{\text{cens}}(t)
=\frac{1}{n_0}\sum_{i:\,\delta_i=0}\mathbf 1\{Y_i\le t\},\qquad t\ge 0.
\end{equation}
By the Glivenko–Cantelli theorem~\cite{tucker1959generalization}, as sample sizes grow, these ECDFs converge uniformly to their respective target distribution functions
\begin{align}
\widehat H_{\text{event}}(t) &\xrightarrow{\text{uniformly in } t} H_{\text{event}}(t) := \Pr(T \le t \mid T \le C) \\
\widehat H_{\text{cens}}(t) &\xrightarrow{\text{uniformly in } t} H_{\text{cens}}(t) := \Pr(C \le t \mid C < T)
\end{align}
In summary, by splitting the data into event and censored subsamples and plotting their empirical distribution functions $\widehat H_{\text{event}}$ and $\widehat H_{\text{cens}}$, we can evaluate model fit within a unified framework along both dimensions.

%\subsection{Posterior predictive CDF}
%\input{MSc_Statistics_Research_Report_paper/section/Methods_subsection/posterior predictive cdf }




