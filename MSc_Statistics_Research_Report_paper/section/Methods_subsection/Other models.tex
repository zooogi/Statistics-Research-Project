

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%



\subsubsection{Weibull Model} \label{Weibull Model}
The Weibull model is a natural generalization of the exponential model and is suitable for describing scenarios where the hazard rate changes monotonically over time. Compared to the exponential model, which assumes a constant hazard rate, the Weibull model introduces a shape parameter $\alpha$, allowing the hazard rate $h(y)$ to increase or decrease with survival time $y$, providing greater flexibility to capture realistic survival or failure patterns.

The hazard function, probability density function, and survival function are given by:
$$
h(y) = \alpha \gamma y^{\alpha - 1}, 
\quad 
f(y) = \alpha \gamma y^{\alpha - 1} \exp(-\gamma y^{\alpha}), 
\quad 
S(y) = \exp(-\gamma y^{\alpha}),
\quad \gamma > 0,
$$
where $\alpha$ is the shape parameter and $\gamma = \exp(\lambda)$ is the scale parameter. When $\alpha > 1$, the hazard rate increases over time (indicating an aging effect); when $\alpha < 1$, the hazard rate decreases over time (indicating early failure); and when $\alpha = 1$, the Weibull model reduces to the exponential model.

Similar to the exponential case, suppose the survival time sample $Y = (Y_1, \ldots, Y_n)$ is independently and identically distributed according to a common Weibull distribution. To further incorporate individual covariate effects, the scale parameter can be modeled as:
$$
\gamma_i = \exp(\mathbf{x}_i^\top \boldsymbol{\beta}),
\quad i = 1, \ldots, n,
$$
where $\mathbf{x}_i$ denotes the covariate vector and $\boldsymbol{\beta}$ is the vector of regression coefficients. The functional forms of the hazard and density remain unchanged, with $\gamma$ replaced by $\gamma_i$.
Assuming independent samples and right-censoring, the full likelihood function is:
$$
L(\boldsymbol{\beta}, \alpha)
= \prod_{i=1}^n 
\big[ f(y_i) \big]^{\nu_i} 
\big[ S(y_i) \big]^{1 - \nu_i}
= \prod_{i=1}^n 
\big[ \alpha \gamma_i y_i^{\alpha - 1} \big]^{\nu_i} 
\exp(-\gamma_i y_i^\alpha).
$$
For the prior specification, $\boldsymbol{\beta}$ can be assigned the same normal prior as in the exponential regression case, while the shape parameter $\alpha$ is commonly assigned a Gamma prior:
$$
\boldsymbol{\beta} \sim N(\boldsymbol{\mu}_0, \Sigma_0), 
\quad 
\alpha \sim \text{Gamma}(a_0, b_0).
$$
Assuming $\boldsymbol{\beta}$ and $\alpha$ are conditionally independent a priori, the joint posterior distribution (up to a normalizing constant) is:
$$
\pi(\boldsymbol{\beta}, \alpha | D)
\propto L(\boldsymbol{\beta}, \alpha)
\, \pi(\boldsymbol{\beta})
\, \pi(\alpha).
$$
Taking logarithms yields the log-posterior kernel:
$$
\log \pi(\boldsymbol{\beta}, \alpha | D)
= \sum_{i=1}^n 
\big[
\nu_i \log \alpha + \nu_i \mathbf{x}_i^\top \boldsymbol{\beta} 
+ \nu_i (\alpha - 1) \log y_i 
- \exp(\mathbf{x}_i^\top \boldsymbol{\beta}) y_i^\alpha
\big]
+ \log \pi(\boldsymbol{\beta}) + \log \pi(\alpha) + \text{const}.
$$
As in the exponential regression case, this posterior distribution does not have a closed form and must be sampled numerically. It can be shown that, given the normal prior for $\boldsymbol{\beta}$ and Gamma prior for $\alpha$, the log-posterior is concave with respect to each parameter, satisfying the log-concavity condition and making ARS or Gibbs sampling applicable for efficient inference.





\subsubsection{Regression exponential}
%%%%%%%%%%%%%%%%%%%
%下面是指数回归模型的贝叶斯推断
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

If introducing covariates $\mathbf{x}_i$, it is common to link the parameters and covariates through a linear regression (\cite{ibrahim2013bayesian}),
$$
\lambda_i = \exp(\mathbf{x}_i^\top \boldsymbol{\beta}),
\quad i = 1, \ldots, n,
$$
Without covariates, the hazard reduces to (\cite{chen2025survival})
$$
\lambda_i = \exp(\beta_0),
$$
representing a single baseline hazard rate for all individuals. With covariates, $\lambda_i$ varies with individual characteristics.

Here $\boldsymbol{\beta}$ are regression coefficients, and the likelihood becomes
\begin{align*}
L(D | \boldsymbol{\beta})
&= \prod_{i=1}^n 
\big[ f(y_i | \boldsymbol{\beta}) \big]^{\nu_i}
\big[ S(y_i | \boldsymbol{\beta}) \big]^{1 - \nu_i} \\
&= \prod_{i=1}^n 
\big[ \lambda_i \exp(-\lambda_i y_i) \big]^{\nu_i}
\big[ \exp(-\lambda_i y_i) \big]^{1 - \nu_i} \\
&= \prod_{i=1}^n 
\lambda_i^{\nu_i} 
\exp(-\lambda_i y_i) \\
&= \prod_{i=1}^n 
\exp\big( \nu_i \mathbf{x}_i^\top \boldsymbol{\beta} \big)
\exp\big( - y_i \exp(\mathbf{x}_i^\top \boldsymbol{\beta}) \big).
\end{align*}
In Bayesian inference, we assign priors $\pi(\theta)$, either weakly informative (e.g. $\pi(\theta)\propto1$) or informative (e.g. $\boldsymbol{\beta} \sim N(\boldsymbol{\mu}_0, \Sigma_0)$). Priors stabilize estimation, especially with small samples.

Overall, Bayesian inference proceeds by specifying the model and prior, computing the posterior
$$
\pi(\theta | D)
\propto
L(D | \theta) \times \pi(\theta),
$$
then summarizing it via means, medians, credible intervals, or posterior predictive distributions.

For example, with a normal prior, the (unnormalized) posterior is
$$
\pi(\boldsymbol{\beta} | D)
\propto
L(D | \boldsymbol{\beta})
\times
\pi(\boldsymbol{\beta}).
$$
With log posterior
$$
\log \pi(\boldsymbol{\beta} | D)
\propto \sum_{i=1}^n
\big[ \nu_i \mathbf{x}_i^\top \boldsymbol{\beta}
- y_i \exp(\mathbf{x}_i^\top \boldsymbol{\beta}) \big]
- \frac{1}{2}
(\boldsymbol{\beta} - \boldsymbol{\mu}_0)^\top
\Sigma_0^{-1}
(\boldsymbol{\beta} - \boldsymbol{\mu}_0).
$$
As this posterior lacks a closed form, MCMC methods are used for sampling (\cite{stats5010006}). Here, we implement it via \texttt{brms} in R.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Extreme Value Model}
The Extreme Value (EV) model can be viewed as a reparameterization of the Weibull model in the log-time domain, which reformulates the hazard function as an exponential-linear form in log time, facilitating direct integration of covariates.

Specifically, if the survival time $T \sim \text{Weibull}(\alpha, \gamma)$, define:
$$
Y = \log T.
$$
Then,
$$
P(Y \le y) 
= P(\log T \le y)
= P(T \le e^y)
= F_T(e^y)
= 1 - \exp\big( - \gamma e^{\alpha y} \big).
$$
Therefore, the cumulative distribution function (CDF) for the EV model is:
$$
F(y) = 1 - \exp\big( - \exp(\lambda + \alpha y) \big), 
\quad \text{where}~\lambda = \log \gamma.
$$
It follows that:
$$
\begin{aligned}
S(y) &= \exp\big( - \exp(\lambda + \alpha y) \big),\\
f(y) &= \alpha \exp(\lambda + \alpha y) \exp\big( - \exp(\lambda + \alpha y) \big),\\
h(y) &= \alpha \exp(\lambda + \alpha y).
\end{aligned}
$$
Compared to the power-form hazard in the Weibull model, the hazard in the EV model becomes exponential-linear with respect to log time and covariates, making the derivation simpler, the interpretation more straightforward, and the link with covariates naturally linear. In this formulation, $Y \in (-\infty, +\infty)$ is no longer constrained to be positive.

With covariates, the location parameter is specified as:
$$
\lambda_i = \mathbf{x}_i^\top \boldsymbol{\beta}, 
\quad i = 1, \ldots, n.
$$
Accordingly,
$$
f(y_i) = \alpha \exp(\alpha y_i + \lambda_i) \exp\big( -\exp(\alpha y_i + \lambda_i) \big), 
\quad
S(y_i) = \exp\big( -\exp(\alpha y_i + \lambda_i) \big).
$$
The observations $\{ Y_i, \nu_i \}_{i=1}^n$ are assumed to be independent and identically distributed (i.i.d.).

The likelihood function is then:
$$
L(\boldsymbol{\beta}, \alpha) 
= \prod_{i=1}^n 
\big[ f(y_i) \big]^{\nu_i} 
\big[ S(y_i) \big]^{1 - \nu_i}
= \prod_{i=1}^n 
\big[ \alpha \exp(\alpha y_i + \lambda_i) \big]^{\nu_i} 
\exp\big( -\exp(\alpha y_i + \lambda_i) \big).
$$
The prior distributions are the same as for the Weibull model:
$$
\boldsymbol{\beta} \sim N(\boldsymbol{\mu}_0, \Sigma_0), 
\quad 
\alpha \sim \text{Gamma}(a_0, b_0).
$$
The joint posterior (up to a normalizing constant) is:
$$
\pi(\boldsymbol{\beta}, \alpha | D) 
\propto L(\boldsymbol{\beta}, \alpha)
\, \pi(\boldsymbol{\beta}) 
\, \pi(\alpha).
$$
The log-kernel of the posterior is:
$$
\log \pi(\boldsymbol{\beta}, \alpha | D)
= \sum_{i=1}^n 
\big[
\nu_i \log \alpha + \nu_i \alpha y_i + \nu_i \lambda_i - \exp(\alpha y_i + \lambda_i)
\big]
+ \log \pi(\boldsymbol{\beta}) + \log \pi(\alpha) + \text{const}.
$$
It can be shown that the Hessian of this log-posterior with respect to both $\boldsymbol{\beta}$ and $\alpha$ is non-positive definite, implying log-concavity, which allows safe and efficient inference using ARS or Gibbs sampling in practice.
